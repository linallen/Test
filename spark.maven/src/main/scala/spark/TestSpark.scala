package spark

object TestSpark {
  def main(args: Array[String]) {
//    val conf = new SparkConf().setAppName("wiki_test") // create a spark config object
//    val sc = new SparkContext(conf) // Create a spark context
//    val data = sc.textFile("/path/to/somedir") // Read files from "somedir" into an RDD of (filename, content) pairs.
//    val tokens = data.map(_.split(" ")) // Split each file into a list of tokens (words).
//    val wordFreq = tokens.map((_, 1)).reduceByKey(_ + _) // Add a count of one to each token, then sum the counts per word type.
//    wordFreq.sortBy(s => s._2).map(x => (x._2, x._1)).top(10) // Get the top 10 words. Swap word and count to sort by count.
  }
}